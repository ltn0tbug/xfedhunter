{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "MY_TIMEZONE = pytz.timezone(\"Asia/Ho_Chi_Minh\")\n",
    "DATASET_DIR = r\"./dataset\"\n",
    "CHECKPOINT_DIR = r\"./checkpoint\"\n",
    "WORKING_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "with open(\"NIDS.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "config['dataset']['path'] = [DATASET_DIR + \"/netflowv1/NF-NF-ToN-IoT.csv\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match config['server']['model']:\n",
    "    case 'CNNmodel':\n",
    "        TASK = 'b'\n",
    "        N_CLASS = 1\n",
    "    case 'DeepFedmodel':\n",
    "        TASK = 's'\n",
    "        N_CLASS = 2\n",
    "    case 'FedTHmodel':\n",
    "        TASK = 'b'\n",
    "        N_CLASS = 1\n",
    "    case 'NFHuntermodel':\n",
    "        TASK = 'b'\n",
    "        N_CLASS = 1\n",
    "    case _:\n",
    "        raise ValueError(\"Somethings wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdlp2.data.pipe.fed import fed_pipe\n",
    "from mdlp2.data.pipe.nfv1 import nfv1_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = config['dataset']['test_size']\n",
    "NUM_CLIENTS = config['server']['n_client']\n",
    "\n",
    "fed_pipeline = fed_pipe(source_type=\"CSV\", split_radio=TEST_SIZE, n_client=NUM_CLIENTS, keep_radio_feature='Label', random_state=123, verbose=False)\n",
    "test_data, client_data = fed_pipeline(config['dataset']['path'])\n",
    "\n",
    "nfv1_pipeline = nfv1_pipe(source_type=\"DataFrame\", task=TASK, verbose=False)\n",
    "np_test_data = nfv1_pipeline(test_data)\n",
    "np_client_data = [nfv1_pipeline(data) for data in client_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sequence_split(input_np, sequence_length:int=16, keep_final:bool=False):\n",
    "    seq_num = len(input_np)//sequence_length\n",
    "    seqs = [input_np[i*sequence_length:(i+1)*sequence_length] for i in range(seq_num)]\n",
    "    return np.stack(seqs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_test_data = [sequence_split(x) for x in np_test_data]\n",
    "\n",
    "for i in range(len(np_client_data)):\n",
    "    np_client_data[i] = [sequence_split(x) for x in np_client_data[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mdlp2.model as model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.legacy.Adam(config['client']['compile']['lr'])\n",
    "metric = ['Accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
    "if np_test_data[1].shape[-1] == 1:\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "elif np_test_data[1].shape[-1] >= 2:\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    raise ValueError(f\"Data have an invalid number of classes ({client_data[1].shape[-1]}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(model_list, config['server']['model'])(input_shape=np_test_data[0].shape[1:], n_classes=N_CLASS)\n",
    "model.compile(optimizer=optim, loss=loss, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdlp2.fed import fed_avg_simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_W, simulation_history = fed_avg_simulator(\"NIDS.yaml\", np_test_data, np_client_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not os.path.exists(os.path.join(CHECKPOINT_DIR, f\"./train/{config['server']['model']}\")):\n",
    "    os.makedirs(os.path.join(CHECKPOINT_DIR, f\"./train\"),exist_ok=True)\n",
    "    \n",
    "\n",
    "with open(os.path.join(CHECKPOINT_DIR, f\"./train/{config['server']['model']}.pkl\"), 'wb') as f:\n",
    "    pickle.dump([global_W, simulation_history], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss Accuracy Precision Recall F1-Score\")\n",
    "for k, v in simulation_history[1][1].items():\n",
    "    print(\"{:.4f}\".format(v[0].numpy()), end=\" \")\n",
    "    print(\"{:.4f} {:.4f} {:.4f} {:.4f}\".format(*v[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
